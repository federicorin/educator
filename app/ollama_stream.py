"""
ollama_stream.py
M√≥dulo para manejo de streaming y comunicaci√≥n con Ollama/APIs externas
Funciona en local (Ollama) y producci√≥n (Groq/Hugging Face)
Procesa correctamente los metadatos y extrae solo el content sin omisiones
"""

import requests
import json
import re
import time
import logging
import os
from typing import Generator, Dict, Any, List, Optional
from datetime import datetime

# Configuraci√≥n de logging
logger = logging.getLogger(__name__)

class OllamaConfig:
    """Configuraci√≥n centralizada para Ollama"""
    def __init__(self, host: str = "localhost", port: int = 11434, timeout: int = 120):  # Timeout m√°s alto para DeepSeek
        self.host = host
        self.port = port
        self.timeout = timeout
        self.base_url = f"http://{host}:{port}"
        self.generate_url = f"{self.base_url}/api/generate"
        self.chat_url = f"{self.base_url}/api/chat"
    
    def __str__(self):
        return f"OllamaConfig(host={self.host}, port={self.port}, timeout={self.timeout})"

# Instancia global de configuraci√≥n (puede ser modificada)
config = OllamaConfig()

def set_ollama_config(host: str = "localhost", port: int = 11434, timeout: int = 120):
    """Configura la conexi√≥n a Ollama (timeout optimizado para DeepSeek)"""
    global config
    config = OllamaConfig(host, port, timeout)
    logger.info(f"Configuraci√≥n Ollama actualizada: {config}")

def is_production() -> bool:
    """Detecta si estamos en producci√≥n"""
    return os.environ.get('PRODUCTION', 'false').lower() == 'true'

def get_external_api_type() -> str:
    """Determina qu√© API externa gratuita usar"""
    # Priorizamos APIs completamente gratuitas sin autenticaci√≥n
    return os.environ.get('API_TYPE', 'huggingface_free')

def limpiar_output(texto: str, preserve_trailing_space: bool = False) -> str:
    """
    Limpia el texto de caracteres ANSI, spinners y formato extra.
    Si preserve_trailing_space es True, NO eliminar√° los espacios finales,
    pero s√≠ eliminar√° saltos de l√≠nea terminales extras.
    """
    if not texto:
        return ""

    # Remover secuencias de escape ANSI
    ansi_escape = re.compile(r'\x1B(?:[@-Z\\-_]|\[[0-?]*[ -/]*[@-~])')
    limpio = ansi_escape.sub('', texto)

    # Remover caracteres de spinner
    spinner_chars = "‚†ã‚†ô‚†π‚†∏‚†º‚†¥‚†¶‚†ß‚†á‚†è"
    limpio = ''.join(c for c in limpio if c not in spinner_chars)

    # Normalizar saltos de l√≠nea m√∫ltiples
    limpio = re.sub(r'\n{3,}', '\n\n', limpio)

    # Si pedimos preservar espacios finales, no usamos strip() sino que solo
    # quitamos saltos de l√≠nea finales, manteniendo espacios.
    if preserve_trailing_space:
        # eliminar √∫nicamente \r y \n terminales, conservar espacios
        return re.sub(r'[\r\n]+$', '', limpio)
    else:
        # comportamiento previo: quitar espacios al inicio y final
        return limpio.strip()


class ExternalAPIClient:
    """Cliente para APIs externas 100% gratuitas sin autenticaci√≥n"""
    
    def __init__(self):
        self.api_type = get_external_api_type()
    
    def _get_free_api_endpoint(self) -> str:
        """Obtiene endpoint de API gratuita seg√∫n configuraci√≥n"""
        if self.api_type == 'huggingface_free':
            return "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"
        elif self.api_type == 'together_free':
            return "https://api.together.xyz/v1/chat/completions"  # Tiene tier gratuito
        else:
            # Default: Hugging Face Inference API (sin auth para modelos p√∫blicos)
            return "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"
    
    def _huggingface_free_request(self, prompt: str) -> requests.Response:
        """Request a Hugging Face Inference API (gratuita, sin auth para modelos p√∫blicos)"""
        url = "https://api-inference.huggingface.co/models/microsoft/DialoGPT-medium"
        headers = {"Content-Type": "application/json"}
        data = {
            "inputs": prompt,
            "parameters": {
                "max_new_tokens": 512,
                "temperature": 0.8,
                "do_sample": True
            }
        }
        return requests.post(url, headers=headers, json=data, timeout=30)
    
    def _mock_ai_response(self, prompt: str) -> str:
        """Genera respuesta educativa cuando las APIs no est√°n disponibles"""
        # An√°lisis b√°sico del prompt para dar respuesta contextual
        prompt_lower = prompt.lower()
        
        if any(word in prompt_lower for word in ['qu√© es', 'que es', 'define', 'explica', 'concepto']):
            return f"""üìö **Respuesta Educativa**

Te ayudo con tu consulta: "{prompt[:100]}..."

Como asistente educativo, puedo explicarte que este es un tema importante que requiere an√°lisis detallado. Te recomiendo:

1. **Investigar fuentes confiables** sobre el tema
2. **Analizar diferentes perspectivas** 
3. **Relacionar con conocimientos previos**
4. **Aplicar el pensamiento cr√≠tico**

üí° **Sugerencia**: Para obtener respuestas m√°s completas y actualizadas, puedes consultar:
- Libros especializados en el tema
- Art√≠culos acad√©micos revisados por pares  
- Recursos educativos de universidades reconocidas

¬øTe gustar√≠a que te ayude a estructurar tu investigaci√≥n sobre este tema?"""

        elif any(word in prompt_lower for word in ['c√≥mo', 'como', 'steps', 'pasos', 'proceso']):
            return f"""üîß **Gu√≠a Paso a Paso**

Para abordar: "{prompt[:100]}..."

**Metodolog√≠a sugerida:**

1. **Planificaci√≥n**: Define claramente los objetivos
2. **Investigaci√≥n**: Recopila informaci√≥n de fuentes confiables  
3. **An√°lisis**: Eval√∫a la informaci√≥n cr√≠ticamente
4. **S√≠ntesis**: Organiza las ideas principales
5. **Aplicaci√≥n**: Implementa lo aprendido
6. **Evaluaci√≥n**: Reflexiona sobre los resultados

üìù **Consejo**: Documenta cada paso para facilitar el aprendizaje y la revisi√≥n posterior.

¬øNecesitas ayuda espec√≠fica con alguno de estos pasos?"""

        else:
            return f"""üéì **Asistente Educativo Disponible**

He recibido tu consulta: "{prompt[:100]}..."

Como educador digital, estoy aqu√≠ para ayudarte a:
- **Comprender conceptos complejos**
- **Desarrollar habilidades de pensamiento cr√≠tico**  
- **Estructurar tu aprendizaje**
- **Encontrar recursos educativos de calidad**

üìö Para brindarte la mejor asistencia educativa, considera reformular tu pregunta incluyendo:
- El contexto espec√≠fico
- Tu nivel de conocimiento previo
- Qu√© esperas aprender

¬øC√≥mo puedo ayudarte mejor con tu proceso de aprendizaje?"""

    def chat_completion(self, messages: List[Dict[str, str]]) -> str:
        """Completa un chat usando APIs gratuitas o respuesta educativa"""
        try:
            user_prompt = messages[-1].get('content', '') if messages else ''
            
            if self.api_type == 'huggingface_free':
                response = self._huggingface_free_request(user_prompt)
                if response.status_code == 200:
                    result = response.json()
                    if isinstance(result, list) and len(result) > 0:
                        generated = result[0].get('generated_text', '')
                        # Limpiar la respuesta removiendo el input original
                        clean_response = generated.replace(user_prompt, '').strip()
                        return clean_response if clean_response else self._mock_ai_response(user_prompt)
                    elif isinstance(result, dict) and 'error' in result:
                        logger.warning(f"HF API error: {result['error']}")
                        return self._mock_ai_response(user_prompt)
                else:
                    logger.warning(f"HF API status: {response.status_code}")
                    return self._mock_ai_response(user_prompt)
            
            # Fallback siempre a respuesta educativa
            return self._mock_ai_response(user_prompt)
        
        except Exception as e:
            logger.error(f"Error en API externa: {e}")
            user_prompt = messages[-1].get('content', '') if messages else ''
            return self._mock_ai_response(user_prompt)
    
    def stream_completion(self, messages: List[Dict[str, str]]) -> Generator[str, None, None]:
        """Stream de chat usando APIs gratuitas o respuesta simulada"""
        try:
            # Para APIs sin streaming nativo, simular stream con respuesta completa
            full_response = self.chat_completion(messages)
            
            # Simular streaming dividiendo en palabras con delay natural
            words = full_response.split(' ')
            for i, word in enumerate(words):
                if i == len(words) - 1:
                    yield word  # √öltima palabra sin espacio
                else:
                    yield word + ' '
                    
        except Exception as e:
            logger.error(f"Error en streaming externo: {e}")
            yield f"Error: {str(e)}"


class ContentProcessor:
    """Procesador de contenido que maneja la extracci√≥n y uni√≥n de texto"""
    
    def __init__(self):
        self.accumulated_content = ""
        self.total_chunks = 0
        self.error_count = 0
        
    def process_chunk(self, raw_chunk: str) -> Optional[str]:
        """
        Procesa un chunk individual extrayendo solo el content
        y agregando espacio al final de cada palabra completa
        """
        if not raw_chunk or not raw_chunk.strip():
            return None
            
        try:
            # Parsear JSON del chunk
            chunk_data = json.loads(raw_chunk.strip())
            
            # Extraer content seg√∫n el formato de respuesta
            content = None
            
            if isinstance(chunk_data, dict):
                # Formato de chat API
                if 'message' in chunk_data and 'content' in chunk_data['message']:
                    content = chunk_data['message']['content']
                # Formato de generate API
                elif 'response' in chunk_data:
                    content = chunk_data['response']
                # Content directo
                elif 'content' in chunk_data:
                    content = chunk_data['content']
                    
                # Verificar si es el chunk final
                if chunk_data.get('done', False):
                    logger.debug(f"Chunk final recibido. Total procesados: {self.total_chunks}")
                    
            if content is not None:
                self.total_chunks += 1
                
                # üî• AGREGAR ESPACIO: Detectar si termina con palabra completa
                processed_content = content + " "
                
                # Acumular contenido original (sin el espacio extra para stats)
                self.accumulated_content += content
                
                # Retornar contenido con espacio para streaming
                return processed_content
                
        except json.JSONDecodeError as e:
            self.error_count += 1
            logger.warning(f"Error parseando chunk JSON: {e}, chunk: {raw_chunk[:100]}...")
            return None
        except Exception as e:
            self.error_count += 1
            logger.error(f"Error procesando chunk: {e}")
            return None
            
        return None
    
    def get_stats(self) -> Dict[str, Any]:
        """Retorna estad√≠sticas del procesamiento"""
        return {
            'total_chunks': self.total_chunks,
            'error_count': self.error_count,
            'content_length': len(self.accumulated_content),
            'accumulated_content': self.accumulated_content
        }

def test_ollama_connection() -> bool:
    """
    Prueba la conexi√≥n con Ollama (solo en modo local)
    """
    if is_production():
        logger.info("‚úÖ Modo producci√≥n - usando API externa")
        return True
        
    try:
        response = requests.get(f"{config.base_url}/api/tags", timeout=5)
        if response.status_code == 200:
            logger.info("‚úÖ Conexi√≥n con Ollama exitosa")
            return True
        else:
            logger.error(f"‚ùå Ollama respondi√≥ con c√≥digo {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        logger.error(f"‚ùå No se pudo conectar a Ollama en {config.base_url}")
        return False
    except Exception as e:
        logger.error(f"‚ùå Error probando conexi√≥n: {e}")
        return False

def build_system_prompt(user_customize_ai: str = "", kb_context: str = "") -> str:
    """
    Construye el prompt del sistema combinando personalizaci√≥n y contexto
    Optimizado para educaci√≥n
    """
    base_prompt = """Eres un asistente educativo experto. Tu funci√≥n es ayudar a estudiantes y educadores proporcionando explicaciones claras, detalladas y pedag√≥gicamente s√≥lidas. 

Caracter√≠sticas de tus respuestas:
- Explica conceptos paso a paso
- Usa ejemplos pr√°cticos y relevantes
- Adapta el nivel de complejidad al contexto
- Fomenta el pensamiento cr√≠tico
- Proporciona recursos adicionales cuando sea apropiado"""
    
    parts = [base_prompt]
    
    if user_customize_ai.strip():
        parts.append(f"Informaci√≥n adicional sobre el usuario: {user_customize_ai.strip()}")
    
    if kb_context.strip():
        parts.append(f"Contexto relevante de la base de conocimientos: {kb_context.strip()}")
    
    return "\n\n".join(parts)

def prepare_messages(user_customize_ai: str, kb_context: str, prompt: str) -> List[Dict[str, str]]:
    """
    Prepara los mensajes para la API de chat
    """
    system_content = build_system_prompt(user_customize_ai, kb_context)
    
    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": prompt}
    ]
    
    return messages

def stream_chat_for_user(
    user_customize_ai: str = "",
    kb_context: str = "",
    prompt: str = "",
    model: str = "deepseek-r1:14b_educator",
    timeout: int = None
) -> Generator[str, None, None]:
    """
    Genera un stream de chat procesando correctamente los metadatos
    y extrayendo solo el content sin omisiones
    """
    if timeout is None:
        timeout = config.timeout
    
    # Preparar mensajes
    messages = prepare_messages(user_customize_ai, kb_context, prompt)
    
    # üî• NUEVA L√ìGICA: Detectar modo producci√≥n
    if is_production():
        logger.info("üåê Usando API externa para streaming")
        external_client = ExternalAPIClient()
        
        try:
            for chunk in external_client.stream_completion(messages):
                if chunk:
                    clean_chunk = limpiar_output(chunk, preserve_trailing_space=True)
                    if clean_chunk:
                        yield clean_chunk
        except Exception as e:
            logger.error(f"Error en streaming externo: {e}")
            yield f"Error: No pude procesar tu consulta. {str(e)}"
        return
    
    # MODO LOCAL - C√≥digo original de Ollama
    # Verificar conexi√≥n
    if not test_ollama_connection():
        raise ConnectionError(f"No se pudo conectar a Ollama en {config.base_url}")
    
    # Preparar payload para la API
    payload = {
        "model": model,
        "messages": messages,
        "stream": True,
        "options": {
            "temperature": 0.8,  # Ligeramente m√°s alta para DeepSeek
            "top_p": 0.95,       # M√°s diversidad para respuestas educativas
            "top_k": 50,         # Mayor variedad de tokens
            "repeat_penalty": 1.1,
            "num_ctx": 4096,     # Contexto extendido para DeepSeek
            "num_predict": -1    # Sin l√≠mite de tokens de predicci√≥n
        }
    }
    
    logger.info(f"Iniciando stream local con modelo {model} hacia {config.chat_url}")
    
    # Procesador de contenido
    processor = ContentProcessor()
    
    try:
        # Realizar request con streaming
        with requests.post(
            config.chat_url,
            json=payload,
            timeout=timeout,
            stream=True
        ) as response:
            
            if response.status_code != 200:
                error_msg = f"Error HTTP {response.status_code}: {response.text}"
                logger.error(error_msg)
                raise Exception(error_msg)
            
            logger.info("Stream iniciado exitosamente")
            
            # Procesar chunks
            for line in response.iter_lines(decode_unicode=True):
                if not line or not line.strip():
                    continue
                
                # Procesar chunk y extraer content
                content = processor.process_chunk(line)
                
                if content is not None:
                    # Limpiar content antes de enviar, preservando espacio final para streaming
                    clean_content = limpiar_output(content, preserve_trailing_space=True)
                    if clean_content:  # Solo enviar si hay contenido despu√©s de limpiar
                        # DEBUG opcional: ver representaci√≥n con espacios
                        logger.debug(f"Chunk limpio repr: {repr(clean_content)}")
                        yield clean_content
            
            # Log de estad√≠sticas finales
            stats = processor.get_stats()
            logger.info(f"Stream completado: {stats['total_chunks']} chunks, "
                       f"{stats['content_length']} caracteres, "
                       f"{stats['error_count']} errores")
                       
    except requests.exceptions.Timeout:
        logger.error(f"Timeout despu√©s de {timeout} segundos")
        raise TimeoutError(f"La request a Ollama excedi√≥ el timeout de {timeout} segundos")
    
    except requests.exceptions.ConnectionError as e:
        logger.error(f"Error de conexi√≥n: {e}")
        raise ConnectionError(f"Error conectando con Ollama: {e}")
    
    except Exception as e:
        logger.error(f"Error inesperado en stream: {e}")
        raise

def chat_once(
    messages: List[Dict[str, str]],
    model: str = "deepseek-r1:14b_educator",
    timeout: int = None
) -> str:
    """
    Realiza una sola consulta sin streaming
    Funciona tanto en local (Ollama) como en producci√≥n (API externa)
    """
    if timeout is None:
        timeout = config.timeout
    
    # üî• NUEVA L√ìGICA: Detectar modo producci√≥n
    if is_production():
        logger.info("üåê Usando API externa para consulta √∫nica")
        external_client = ExternalAPIClient()
        try:
            response = external_client.chat_completion(messages)
            return limpiar_output(response)
        except Exception as e:
            logger.error(f"Error en consulta externa: {e}")
            return f"Error: No pude procesar tu consulta. {str(e)}"
    
    # MODO LOCAL - C√≥digo original de Ollama
    # Verificar conexi√≥n
    if not test_ollama_connection():
        raise ConnectionError(f"No se pudo conectar a Ollama en {config.base_url}")
    
    payload = {
        "model": model,
        "messages": messages,
        "stream": False,
        "options": {
            "temperature": 0.8,  # Configuraci√≥n optimizada para DeepSeek
            "top_p": 0.95,
            "top_k": 50,
            "repeat_penalty": 1.1,
            "num_ctx": 4096,
            "num_predict": -1
        }
    }
    
    logger.info(f"Realizando consulta √∫nica con modelo {model}")
    
    try:
        response = requests.post(
            config.chat_url,
            json=payload,
            timeout=timeout
        )
        
        if response.status_code != 200:
            error_msg = f"Error HTTP {response.status_code}: {response.text}"
            logger.error(error_msg)
            raise Exception(error_msg)
        
        result = response.json()
        
        # Extraer content de la respuesta
        if 'message' in result and 'content' in result['message']:
            content = result['message']['content']
        elif 'response' in result:
            content = result['response']
        else:
            logger.warning(f"Formato de respuesta inesperado: {result}")
            content = str(result)
        
        # Limpiar y retornar
        clean_content = limpiar_output(content)
        logger.info(f"Consulta √∫nica completada: {len(clean_content)} caracteres")
        
        return clean_content
        
    except requests.exceptions.Timeout:
        logger.error(f"Timeout despu√©s de {timeout} segundos")
        raise TimeoutError(f"La request a Ollama excedi√≥ el timeout de {timeout} segundos")
    
    except requests.exceptions.ConnectionError as e:
        logger.error(f"Error de conexi√≥n: {e}")
        raise ConnectionError(f"Error conectando con Ollama: {e}")
    
    except Exception as e:
        logger.error(f"Error inesperado: {e}")
        raise

def set_debug_mode(enabled: bool = True):
    """
    Habilita/deshabilita el modo debug para logging detallado
    """
    level = logging.DEBUG if enabled else logging.INFO
    logging.getLogger(__name__).setLevel(level)
    logger.info(f"Modo debug {'habilitado' if enabled else 'deshabilitado'}")

def get_available_models() -> List[str]:
    """
    Obtiene la lista de modelos disponibles
    """
    if is_production():
        # En producci√≥n, retornar modelos de APIs gratuitas
        return ['microsoft/DialoGPT-medium', 'educational-assistant-free', 'mock-educator']
    
    # Modo local - Ollama
    try:
        response = requests.get(f"{config.base_url}/api/tags", timeout=10)
        if response.status_code == 200:
            data = response.json()
            models = [model['name'] for model in data.get('models', [])]
            logger.info(f"Modelos disponibles: {models}")
            return models
        else:
            logger.error(f"Error obteniendo modelos: HTTP {response.status_code}")
            return []
    except Exception as e:
        logger.error(f"Error conectando para obtener modelos: {e}")
        return []

# Funci√≥n especial para KnowledgeBase
def ollama_run_for_kb(model: str, prompt: str) -> str:
    """
    Funci√≥n para reemplazar subprocess en KnowledgeBase
    Simula 'ollama run' pero usando nuestro sistema dual
    """
    try:
        if is_production():
            # En producci√≥n, usar API externa
            external_client = ExternalAPIClient()
            messages = [{"role": "user", "content": prompt}]
            response = external_client.chat_completion(messages)
            return response
        else:
            # En local, usar Ollama real
            messages = [{"role": "user", "content": prompt}]
            response = chat_once(messages, model=model)
            return response
    except Exception as e:
        logger.error(f"Error en ollama_run_for_kb: {e}")
        return f"‚ö†Ô∏è Error al procesar consulta: {e}"

# Funci√≥n de utilidad para testing
def test_stream_functionality(prompt: str = "Explica el concepto de fotos√≠ntesis de manera did√°ctica", model: str = "deepseek-r1:14b_educator"):
    """
    Funci√≥n de prueba para verificar que el streaming funciona correctamente
    """
    print(f"üß™ Probando streaming con prompt: '{prompt}'")
    print(f"üîß Modo producci√≥n: {is_production()}")
    if not is_production():
        print(f"üîß Configuraci√≥n local: {config}")
    else:
        print(f"üîß API externa: {get_external_api_type()}")
    
    try:
        accumulated = ""
        chunk_count = 0
        
        for chunk in stream_chat_for_user(prompt=prompt, model=model):
            accumulated += chunk
            chunk_count += 1
            print(f"Chunk #{chunk_count}: {repr(chunk)}")
        
        print(f"\n‚úÖ Prueba completada:")
        print(f"   - Chunks recibidos: {chunk_count}")
        print(f"   - Contenido total: {len(accumulated)} caracteres")
        print(f"   - Respuesta: {accumulated[:200]}...")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error en prueba: {e}")
        return False

if __name__ == "__main__":
    # Configurar logging para pruebas
    logging.basicConfig(level=logging.INFO)
    
    # Configurar Ollama (solo para modo local)
    set_ollama_config(host="localhost", port=11434, timeout=120)  # Timeout m√°s alto para DeepSeek
    
    # Ejecutar prueba
    set_debug_mode(True)
    test_stream_functionality()
